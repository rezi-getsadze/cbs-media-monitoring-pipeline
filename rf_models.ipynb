{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900927b87563e3d4",
   "metadata": {},
   "source": [
    "# Prerequisite\n",
    "\n",
    "When u are not familiar with -> `pip install -r requirements.txt`, use this codeblock for installing the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:37:41.335968Z",
     "start_time": "2024-11-11T16:37:41.332492Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, precision_recall_curve\n",
    "import sklearn.metrics as metrics\n",
    "import pickle\n",
    "import os\n",
    "import subprocess\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28101c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jupyter\n",
    "# !pip install scikit-learn\n",
    "# !pip install pandas\n",
    "# !pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a72ee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one would not work with the requirements txt, so run it once separately:\n",
    "# Download spaCy Dutch Model\n",
    "try:\n",
    "    import spacy\n",
    "    if not spacy.util.is_package(\"nl_core_news_lg\"):\n",
    "        print(\"Downloading spaCy Dutch model.\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"nl_core_news_lg\"])\n",
    "except ImportError:\n",
    "    print(\"spaCy not found. Please run 'pip install spacy' first.\")\n",
    "\n",
    "# Download NLTK dependencies for legacy stemming/stopwords\n",
    "try:\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "except ImportError:\n",
    "    print(\"NLTK not found. Please run 'pip install nltk' first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6ede34d6221574",
   "metadata": {},
   "source": [
    "# Functions for evaluation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a27318583189b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:28:49.183797Z",
     "start_time": "2024-11-11T16:28:49.178804Z"
    }
   },
   "outputs": [],
   "source": [
    "def resultClassifierfloat(row):\n",
    "    threshold = 0.5\n",
    "    if (row['prediction'] > threshold and row['label'] == True):\n",
    "        return 'TP'\n",
    "    if (row['prediction'] < threshold and row['label'] == False):\n",
    "        return 'TN'\n",
    "    if (row['prediction'] < threshold and row['label'] == True):\n",
    "        return 'FN'\n",
    "    if (row['prediction'] > threshold and row['label'] == False):\n",
    "        return 'FP'\n",
    "\n",
    "\n",
    "def resultClassifierint(row):\n",
    "    if (row['label'] == row['prediction'] and row['label'] == True):\n",
    "        return 'TP'\n",
    "    if (row['label'] == row['prediction'] and row['label'] == False):\n",
    "        return 'TN'\n",
    "    if (row['label'] != row['prediction'] and row['label'] == True):\n",
    "        return 'FN'\n",
    "    if (row['label'] != row['prediction'] and row['label'] == False):\n",
    "        return 'FP'\n",
    "\n",
    "# ! CHANGED THIS TO HANDLE 0 FP\n",
    "def evaluation(model, name, X_test, y_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    probs = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(f\"\\n--- {name} Results ---\")\n",
    "    print(f\"Accuracy:  {accuracy_score(y_test, predictions):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, predictions):.4f}\")\n",
    "    print(f\"Recall:    {recall_score(y_test, predictions):.4f}\")\n",
    "    print(f\"F1 Score:  {f1_score(y_test, predictions):.4f}\")\n",
    "    print(f\"AUC Score: {roc_auc_score(y_test, probs):.4f}\")\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "    print(f\"Matrix:    TP={tp}, FP={fp}, TN={tn}, FN={fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c683f9ccab8d02a",
   "metadata": {},
   "source": [
    "# Load preprocessed data set\n",
    "\n",
    "Split into train, test etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462480f44a919ad3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:37:41.841031Z",
     "start_time": "2024-11-11T16:37:41.798682Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Loading dataset.\")\n",
    "# Load the preprocessed file (which now already contains the '%' column)\n",
    "final_trainset = pd.read_csv('trainset_reconstructed.csv').fillna(0)\n",
    "\n",
    "# Filter for high-confidence matches (>88%) to remove noise\n",
    "# Convert score to numeric (just in case)\n",
    "final_trainset['%'] = pd.to_numeric(final_trainset['%'], errors='coerce').fillna(0)\n",
    "\n",
    "# Apply the 0.88 Threshold\n",
    "# We keep matches > 0.88. We keep ALL non-matches (0).\n",
    "clean_positives = final_trainset[(final_trainset['match'] == 1) & (final_trainset['%'] >= 0.88)]\n",
    "clean_negatives = final_trainset[final_trainset['match'] == 0]\n",
    "\n",
    "# Re-balance negatives to 50/50 (Downsample to match the new, smaller positive count)\n",
    "if len(clean_positives) > 0:\n",
    "    clean_negatives = clean_negatives.sample(n=len(clean_positives), random_state=42)\n",
    "    final_trainset = pd.concat([clean_positives, clean_negatives]).sample(frac=1, random_state=42).fillna(0)\n",
    "    print(f\"Filtered Dataset Size: {len(final_trainset)} rows.\")\n",
    "else:\n",
    "    print(\"No matches found above the threshold.\")\n",
    "\n",
    "# Define the target\n",
    "target = 'match'\n",
    "\n",
    "# Split the data into 80% train and 20% test (using GroupShuffleSplit to prevent data leak)\n",
    "splitter = GroupShuffleSplit(test_size=0.20, n_splits=1, random_state=42)\n",
    "train_inds, test_inds = next(splitter.split(final_trainset, groups=final_trainset['child_id']))\n",
    "\n",
    "train_set = final_trainset.iloc[train_inds]\n",
    "test_set = final_trainset.iloc[test_inds]\n",
    "\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Test set size: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a9e7085b41ad0",
   "metadata": {},
   "source": [
    "# Traininig Phase (English / Initial Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81d5ca64a063f6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:41:04.591226Z",
     "start_time": "2024-11-11T16:41:04.587233Z"
    }
   },
   "outputs": [],
   "source": [
    "# Legacy features\n",
    "feature_cols_legacy = ['title_sim_legacy', 'content_sim_legacy', 'date_binary']\n",
    "\n",
    "X_train_legacy = train_set[feature_cols_legacy]\n",
    "y_train = train_set[target]\n",
    "\n",
    "X_test_legacy = test_set[feature_cols_legacy]\n",
    "y_test = test_set[target]\n",
    "\n",
    "# Random forest (same settings as before)\n",
    "rf_legacy = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf_legacy.fit(X_train_legacy, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a8e861c7deb36c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:43:43.726011Z",
     "start_time": "2024-11-11T16:43:43.540477Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "filename = 'model_rf_initial.pkl'\n",
    "pickle.dump(rf_legacy, open(filename, 'wb'))\n",
    "print(f\"Saved: {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3586f244c645957c",
   "metadata": {},
   "source": [
    "# Training Phase (Dutch / Fixed Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11629c6fc7c14494",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:46:27.158547Z",
     "start_time": "2024-11-11T16:46:27.110174Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dutch features\n",
    "feature_cols_dutch = ['title_sim_dutch', 'content_sim_dutch', 'date_binary']\n",
    "\n",
    "X_train_dutch = train_set[feature_cols_dutch]\n",
    "y_train = train_set[target]\n",
    "\n",
    "X_test_dutch = test_set[feature_cols_dutch]\n",
    "y_test = test_set[target]\n",
    "\n",
    "# Same random forest\n",
    "rf_dutch = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf_dutch.fit(X_train_dutch, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ba675",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = 'model_rf_fixed.pkl'\n",
    "pickle.dump(rf_dutch, open(filename, 'wb'))\n",
    "print(f\"Saved: {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fdbe591f55f9ff",
   "metadata": {},
   "source": [
    "# Evaluation Phase (Both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6994b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(rf_legacy, \"Legacy Model (Jaccard/Stemming)\", X_test_legacy, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be938fd57ae79e06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:47:59.037314Z",
     "start_time": "2024-11-11T16:47:58.982241Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluation(rf_dutch, \"New Model (Dutch Vectors)\", X_test_dutch, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd24c9ab",
   "metadata": {},
   "source": [
    "Comparing the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee3e29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probabilities for the curve\n",
    "probs_legacy = rf_legacy.predict_proba(X_test_legacy)[:, 1]\n",
    "probs_dutch = rf_dutch.predict_proba(X_test_dutch)[:, 1]\n",
    "\n",
    "# Plot ROC Curve\n",
    "fpr_a, tpr_a, _ = roc_curve(y_test, probs_legacy)\n",
    "fpr_b, tpr_b, _ = roc_curve(y_test, probs_dutch)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Subplot ROC\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "# Change color to 'red' and linewidth to 3 so you can't miss it\n",
    "plt.plot(fpr_a, tpr_a, label=f'Legacy Model (AUC: {auc(fpr_a, tpr_a):.3f})', linestyle='-', color='red', linewidth=3.5)\n",
    "plt.plot(fpr_b, tpr_b, label=f'New Dutch Model (AUC: {auc(fpr_b, tpr_b):.3f})', color='blue', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Reference (AUC = 0.5)', alpha=0.5)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve (Accuracy Comparison)')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Subplot Precision-0Recall\n",
    "prec_a, rec_a, _ = precision_recall_curve(y_test, probs_legacy)\n",
    "prec_b, rec_b, _ = precision_recall_curve(y_test, probs_dutch)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(rec_a, prec_a, label='Legacy Model', linestyle='-', color='red', linewidth=3.5)\n",
    "plt.plot(rec_b, prec_b, label='New Dutch Model', color='blue', linewidth=2)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve (Reliability)')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('model_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ebeeee",
   "metadata": {},
   "source": [
    "# Overfitting check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9532ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_overfitting(model, name, X_train, y_train, X_test, y_test):\n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    \n",
    "    print(f\"--- Overfitting Check: {name} ---\")\n",
    "    print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Test Accuracy:     {test_acc:.4f}\")\n",
    "    print(f\"Difference:        {train_acc - test_acc:.4f}\")\n",
    "    if (train_acc - test_acc) > 0.05:\n",
    "        print(\"Warning: Model might be overfitting (Gap > 5%)\")\n",
    "    else:\n",
    "        print(\"Model generalization looks healthy.\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# Check Legacy Model\n",
    "check_overfitting(rf_legacy, \"Legacy Model\", X_train_legacy, y_train, X_test_legacy, y_test)\n",
    "\n",
    "# Check Dutch Vector Model\n",
    "check_overfitting(rf_dutch, \"Dutch Vector Model\", X_train_dutch, y_train, X_test_dutch, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb317a05",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb77cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting pandas to show full text\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "# loading the big csv files once so we dont have to keep doing it\n",
    "print(\"loading raw text files.\")\n",
    "# using just the cols we need\n",
    "children_df = pd.read_csv('data/full_children.csv', usecols=['id', 'title', 'content'])\n",
    "parents_df = pd.read_csv('data/full_parents.csv', usecols=['id', 'title', 'content'])\n",
    "\n",
    "# renaming to make the merge easy\n",
    "children_df = children_df.rename(columns={'id': 'child_id', 'title': 'child_title', 'content': 'child_content'})\n",
    "parents_df = parents_df.rename(columns={'id': 'parent_id', 'title': 'parent_title', 'content': 'parent_content'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908edb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- LEGACY MODEL: CORRECT PREDICTIONS ---\")\n",
    "\n",
    "# loading the legacy model\n",
    "with open('model_rf_initial.pkl', 'rb') as f:\n",
    "    model_legacy = pickle.load(f)\n",
    "\n",
    "# predicting\n",
    "cols_legacy = ['title_sim_legacy', 'content_sim_legacy', 'date_binary']\n",
    "preds_legacy = model_legacy.predict(test_set[cols_legacy])\n",
    "df_legacy = test_set.copy()\n",
    "df_legacy['pred'] = preds_legacy\n",
    "\n",
    "# filtering for correct stuff\n",
    "tp_legacy = df_legacy[(df_legacy['pred'] == 1) & (df_legacy['match'] == 1)] # true positives\n",
    "tn_legacy = df_legacy[(df_legacy['pred'] == 0) & (df_legacy['match'] == 0)] # true negatives\n",
    "\n",
    "print(f\"True Positives (hits): {len(tp_legacy)}\")\n",
    "print(f\"True Negatives (correct rejections): {len(tn_legacy)}\")\n",
    "\n",
    "# displaying content for TP\n",
    "print(\"\\nTop 5 True Positives (Matches it found):\")\n",
    "tp_text = tp_legacy.merge(children_df, on='child_id', how='left').merge(parents_df, on='parent_id', how='left')\n",
    "display(tp_text[['child_id', 'parent_id', 'title_sim_legacy', 'child_title', 'parent_title', 'child_content', 'parent_content']].head())\n",
    "\n",
    "# displaying content for TN\n",
    "print(\"\\nTop 5 True Negatives (Correctly said NO):\")\n",
    "tn_text = tn_legacy.merge(children_df, on='child_id', how='left').merge(parents_df, on='parent_id', how='left')\n",
    "display(tn_text[['child_id', 'parent_id', 'title_sim_legacy', 'child_title', 'parent_title', 'child_content', 'parent_content']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67296309",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- LEGACY MODEL: ERRORS ---\")\n",
    "\n",
    "# we already have the predictions from the cell above in 'df_legacy'\n",
    "\n",
    "# filtering for errors\n",
    "fp_legacy = df_legacy[(df_legacy['pred'] == 1) & (df_legacy['match'] == 0)] # false pos\n",
    "fn_legacy = df_legacy[(df_legacy['pred'] == 0) & (df_legacy['match'] == 1)] # false neg (misses)\n",
    "\n",
    "print(f\"False Positives: {len(fp_legacy)}\")\n",
    "print(f\"False Negatives: {len(fn_legacy)}\")\n",
    "\n",
    "# displaying FP\n",
    "print(\"\\nTop 5 False Positives (Hallucinations?):\")\n",
    "if len(fp_legacy) > 0:\n",
    "    fp_text = fp_legacy.merge(children_df, on='child_id', how='left').merge(parents_df, on='parent_id', how='left')\n",
    "    display(fp_text[['child_id', 'parent_id', 'title_sim_legacy', 'child_title', 'parent_title', 'child_content', 'parent_content']].head())\n",
    "else:\n",
    "    print(\"none found\")\n",
    "\n",
    "# displaying FN\n",
    "print(\"\\nTop 5 False Negatives (Missed matches):\")\n",
    "if len(fn_legacy) > 0:\n",
    "    fn_text = fn_legacy.merge(children_df, on='child_id', how='left').merge(parents_df, on='parent_id', how='left')\n",
    "    display(fn_text[['child_id', 'parent_id', 'title_sim_legacy', 'child_title', 'parent_title', 'child_content', 'parent_content']].head())\n",
    "else:\n",
    "    print(\"none found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073528b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- DUTCH MODEL: CORRECT PREDICTIONS ---\")\n",
    "\n",
    "# loading the new dutch model\n",
    "with open('model_rf_fixed.pkl', 'rb') as f:\n",
    "    model_dutch = pickle.load(f)\n",
    "\n",
    "# predicting\n",
    "cols_dutch = ['title_sim_dutch', 'content_sim_dutch', 'date_binary']\n",
    "preds_dutch = model_dutch.predict(test_set[cols_dutch])\n",
    "df_dutch = test_set.copy()\n",
    "df_dutch['pred'] = preds_dutch\n",
    "\n",
    "# filtering correct ones\n",
    "tp_dutch = df_dutch[(df_dutch['pred'] == 1) & (df_dutch['match'] == 1)]\n",
    "tn_dutch = df_dutch[(df_dutch['pred'] == 0) & (df_dutch['match'] == 0)]\n",
    "\n",
    "print(f\"True Positives (hits): {len(tp_dutch)}\")\n",
    "print(f\"True Negatives (correct rejections): {len(tn_dutch)}\")\n",
    "\n",
    "# showing TP\n",
    "print(\"\\nTop 5 True Positives (Matches it found):\")\n",
    "tp_dutch_text = tp_dutch.merge(children_df, on='child_id', how='left').merge(parents_df, on='parent_id', how='left')\n",
    "display(tp_dutch_text[['child_id', 'parent_id', 'title_sim_dutch', 'child_title', 'parent_title', 'child_content', 'parent_content']].head())\n",
    "\n",
    "# showing TN\n",
    "print(\"\\nTop 5 True Negatives (Correctly said NO):\")\n",
    "tn_dutch_text = tn_dutch.merge(children_df, on='child_id', how='left').merge(parents_df, on='parent_id', how='left')\n",
    "display(tn_dutch_text[['child_id', 'parent_id', 'title_sim_dutch', 'child_title', 'parent_title', 'child_content', 'parent_content']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e9bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- DUTCH MODEL: ERRORS ---\")\n",
    "\n",
    "# using predictions from previous cell\n",
    "\n",
    "# filtering errors\n",
    "fp_dutch = df_dutch[(df_dutch['pred'] == 1) & (df_dutch['match'] == 0)]\n",
    "fn_dutch = df_dutch[(df_dutch['pred'] == 0) & (df_dutch['match'] == 1)]\n",
    "\n",
    "print(f\"False Positives: {len(fp_dutch)}\")\n",
    "print(f\"False Negatives: {len(fn_dutch)}\")\n",
    "\n",
    "# showing FP\n",
    "print(\"\\nTop 5 False Positives (Checking for hidden matches):\")\n",
    "if len(fp_dutch) > 0:\n",
    "    fp_dutch_text = fp_dutch.merge(children_df, on='child_id', how='left').merge(parents_df, on='parent_id', how='left')\n",
    "    display(fp_dutch_text[['child_id', 'parent_id', 'title_sim_dutch', 'child_title', 'parent_title', 'child_content', 'parent_content']].head())\n",
    "else:\n",
    "    print(\"none found\")\n",
    "\n",
    "# showing FN\n",
    "print(\"\\nTop 5 False Negatives (Missed matches):\")\n",
    "if len(fn_dutch) > 0:\n",
    "    fn_dutch_text = fn_dutch.merge(children_df, on='child_id', how='left').merge(parents_df, on='parent_id', how='left')\n",
    "    display(fn_dutch_text[['child_id', 'parent_id', 'title_sim_dutch', 'child_title', 'parent_title', 'child_content', 'parent_content']].head())\n",
    "else:\n",
    "    print(\"none found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873b5444",
   "metadata": {},
   "source": [
    "Looking at the actual articles where the models messed up (False Positives and False Negatives) to see what's actually going on:\n",
    "\n",
    "### 1. The Legacy Model is kind of \"blind\"\n",
    "The model misses obvious matches because it's too focused on exact words:\n",
    "* Example (False Negative): child_id 0 vs parent_id 658211\n",
    "* Child Article: \"High surplus value for homeowners\" (Hoog bedrag aan overwaarde)\n",
    "* Parent Article: \"Homes almost 9% more expensive\" (Koopwoningen bijna 9 procent duurder)\n",
    "\n",
    "Both articles are clearly about the housing market and rising prices. But because the titles didn't share exact keywords (one said \"surplus\" other \"expensive\"), the legacy model gave it a score of 0 - it was completely blind to the connection. This confirms that old Jaccard approach has ehh Recall and ignores stuff that isn't basically a copy-paste.\n",
    "\n",
    "### 2. The New Model: Hallucinations vs. Hidden Gems\n",
    "The new Dutch Vector model fixes the blindness, but it introduces a new kinds of issues lol.\n",
    "\n",
    "Bad part: it gets confused by numbers (Hallucinations)\n",
    "Because every CBS article is written in the same \"bureaucratic style,\" the model sometimes thinks different stats are the same thing.\n",
    "* Example (False Positive): child_id 1120849 vs parent_id 916785\n",
    "* Child: \"Inflation fell to 9.9 percent\"\n",
    "* Parent: \"Industrial production grows by nearly 10 percent\"\n",
    "\n",
    "My model gave this a high score (0.72). It probably say \"9.9\" or \"10\" percents, and words like \"Growth/Decline\" and got too excited to make the match. It struggles to tell the difference between types of economic stats when the sentence structure is identical.\n",
    "\n",
    "Good part: it found matches the og trainset (whatever code was used to do the matching) missed (Hidden Gems in a sense). So, in a sense, the model is smart.\n",
    "* Example (False Positive): child_id 4129186 vs parent_id 667719\n",
    "* Child: \"CBS figures on inflow/outflow employees in care...\"\n",
    "* Parent: \"Labor market care and welfare\"\n",
    "\n",
    "The dataset labeled this as a \"Non-Match\" (0), but my model gave it a 0.72. Reading the text, both explicitly discuss the \"1.4 million employees\" in the care sector. The model was right, the label was wrong.\n",
    "\n",
    "### Comparsion / Conclusion\n",
    "On that Housing example where the Legacy model scored 0, the Dutch model gave it a 0.39. It didn't quite hit the match threshold, but at least it saw something. It wasn't blind.\n",
    "\n",
    "* Legacy Model: \"Safe but blind\" (high precision, but misses anything with synonyms).\n",
    "* New Dutch Model: \"Smart but easily confused\" (finds the hidden meanings and corrects human labeling errors, but struggles to differentiate between different statistics because the writing style is so similar).\n",
    "\n",
    "Recommendation: We need to keep the threshold high (like the >0.88 rule) to filter out the \"Statistical Hallucinations,\" while still catching the \"Hidden Gems\" that the old system missed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e3bba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a3a9a01",
   "metadata": {},
   "source": [
    "# Feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1c818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the original model again\n",
    "with open('model_rf_initial.pkl', 'rb') as f:\n",
    "    model_legacy = pickle.load(f)\n",
    "\n",
    "# these are the features we trained on\n",
    "features_legacy = ['title_sim_legacy', 'content_sim_legacy', 'date_binary']\n",
    "\n",
    "# getting the importance scores\n",
    "importances_legacy = model_legacy.feature_importances_\n",
    "\n",
    "# making a dataframe so its easy to read\n",
    "feature_df_legacy = pd.DataFrame({\n",
    "    'feature': features_legacy,\n",
    "    'importance': importances_legacy\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance - Legacy Model:\")\n",
    "print(feature_df_legacy)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(feature_df_legacy['feature'], feature_df_legacy['importance'], color=['#9467bd', '#8c564b', '#e377c2'])\n",
    "plt.title(\"Feature Importance - Legacy Model (Initial)\")\n",
    "plt.ylabel(\"Importance Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c01fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dutch model again\n",
    "with open('model_rf_fixed.pkl', 'rb') as f:\n",
    "    model_dutch = pickle.load(f)\n",
    "\n",
    "# these are the features we trained on\n",
    "features = ['title_sim_dutch', 'content_sim_dutch', 'date_binary']\n",
    "\n",
    "# getting the importance scores\n",
    "importances = model_dutch.feature_importances_\n",
    "\n",
    "# making a dataframe so its easy to read\n",
    "feature_df = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (What the model actually cares about):\")\n",
    "print(feature_df)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(feature_df['feature'], feature_df['importance'], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "plt.title(\"Feature Importance - Dutch Model\")\n",
    "plt.ylabel(\"Importance Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce7c306",
   "metadata": {},
   "source": [
    "Both are kinda lazy.\n",
    "\n",
    "### both models are obsessed with dates\n",
    "`date_binary` is by far the most important feature for both systems.\n",
    "* legacy model: ~74% importance on date\n",
    "* dutch model: ~79% importance on date\n",
    "\n",
    "Basically, both models learned a *shortcut* - metadata, \"if these articles were published on the same day, they are probably a match.\" if the dates don't match, they almost definitely say no.\n",
    "\n",
    "### hallucinations explained (the traffic vs inflation error)\n",
    "since both models rely so much on the date, the difference comes down to how they handle the text.\n",
    "* legacy model: if the date matches, it checks for exact words. if it finds none (score 0), it rejects the match - plays it **saf4e**\n",
    "* Dutch Model: if the date matches, it checks for context/vibe (vectors). As menitoned, CBS articles use similar jargon (\"percent\", \"increase\", \"statistics\"), so vector score is rarely zero, it’s usually like 0.3 or 0.4.\n",
    "\n",
    "So, the dutch model sees a date match + a \"meh\" text score (0.4), so it gets over-confident and marks it as a match. This explains why it matched the traffic deaths with inflation stats—they happened in the same week and sounded bureaucratic.\n",
    "\n",
    "### POTENTIAL FIX: raise the threshold\n",
    "So, at the default threshold (0.5), the model is too excited and makes those \"traffic vs inflation\" errors. But what if we raise the threshold? To 0.8 for example - we could filter out the lazy date-matches (which usually score around 0.4-0.6) and only keep the strong semantic matches (like the housing market example)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2817f94d",
   "metadata": {},
   "source": [
    "# THRESHOLD EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4773636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- COMPARING LEGACY VS STRICT DUTCH ---\n",
    "print(\"\\n--- 1. LEGACY MODEL PERFORMANCE (BASELINE) ---\")\n",
    "\n",
    "# getting legacy predictions (standard threshold 0.5)\n",
    "probs_legacy = model_legacy.predict_proba(test_set[cols_legacy])[:, 1]\n",
    "legacy_preds = (probs_legacy >= 0.5).astype(int)\n",
    "\n",
    "# saving to df to check errors\n",
    "df_legacy_check = test_set.copy()\n",
    "df_legacy_check['pred'] = legacy_preds\n",
    "\n",
    "fp_leg = df_legacy_check[(df_legacy_check['pred'] == 1) & (df_legacy_check['match'] == 0)]\n",
    "fn_leg = df_legacy_check[(df_legacy_check['pred'] == 0) & (df_legacy_check['match'] == 1)]\n",
    "tp_leg = df_legacy_check[(df_legacy_check['pred'] == 1) & (df_legacy_check['match'] == 1)]\n",
    "\n",
    "print(f\"False positives (hallucinations): {len(fp_leg)}\")\n",
    "print(f\"False negatives (misses): {len(fn_leg)}\")\n",
    "print(f\"True positives (hits): {len(tp_leg)}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- 2. DUTCH MODEL WITH STRICT THRESHOLD (0.8) ---\")\n",
    "\n",
    "# getting the probabilities from the dutch model\n",
    "# we only care about the probability of it being a match (column 1)\n",
    "probs_dutch = model_dutch.predict_proba(test_set[cols_dutch])[:, 1]\n",
    "\n",
    "# applying the new rule: only say 'yes' if the score is higher than 0.8\n",
    "# this kills the lazy date-matches because they usually score around 0.4-0.7\n",
    "strict_preds = (probs_dutch >= 0.8).astype(int)\n",
    "\n",
    "# saving results to a new dataframe\n",
    "df_strict = test_set.copy()\n",
    "df_strict['pred'] = strict_preds\n",
    "\n",
    "# recalculating errors\n",
    "fp_strict = df_strict[(df_strict['pred'] == 1) & (df_strict['match'] == 0)]\n",
    "fn_strict = df_strict[(df_strict['pred'] == 0) & (df_strict['match'] == 1)]\n",
    "tp_strict = df_strict[(df_strict['pred'] == 1) & (df_strict['match'] == 1)]\n",
    "\n",
    "print(f\"False positives (hallucinations): {len(fp_strict)} (dropped from 61)\")\n",
    "print(f\"False negatives (misses): {len(fn_strict)}\") \n",
    "print(f\"True positives (hits): {len(tp_strict)}\")\n",
    "\n",
    "# verifying if the specific 'inflation vs production' error is gone\n",
    "# we know these IDs from the 'Top 5 False Positives' table we generated earlier\n",
    "target_child = 1208491 # \"Inflation fell...\"\n",
    "target_parent = 916785 # \"Production grows...\"\n",
    "\n",
    "# checking what the model predicts for this specific pair now\n",
    "check_error = df_strict[(df_strict['child_id'] == target_child) & (df_strict['parent_id'] == target_parent)]\n",
    "\n",
    "if not check_error.empty:\n",
    "    is_match = check_error.iloc[0]['pred']\n",
    "    # if prediction is 0, it means the model said \"NO MATCH\"\n",
    "    print(f\"\\nDid we fix the inflation error (IDs {target_child} & {target_parent})?\")\n",
    "    print(f\"Prediction is now: {is_match} ({'FIXED: Model says NO' if is_match == 0 else 'FAIL: Model says YES'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6a6346",
   "metadata": {},
   "source": [
    "A stricter threshold:\n",
    "\n",
    "1. Kills the lazy errors\n",
    "\n",
    "The fix worked exactly like it was supposed to - the specific error where it confused inflation with industrial production is completely gone now. Total hallucinations dropped by more than half, going from 61 down to 30. This basically proves that a lot of those errors were indeed just \"lazy date matches\" where the model wasn't totally sure (score was probably 0.6 or 0.7) but guessed yes anyway. By raising the bar to 0.8, we forced it to stop guessing on those low-confidence pairs.\n",
    "\n",
    "2. Trade-off\n",
    "\n",
    "To get rid of the noise, we had to sacrifice some real matches. Our false negatives went up from 265 to 315, which means we lost about 50 real matches. These were likely weak matches where the articles were technically related but didn't have strong vector overlap, so the model wasn't confident enough to pass the new 0.8 bar.\n",
    "\n",
    "3. The verdict\n",
    "\n",
    "Honestly this is good enough, better to be safe than sorry in this case. In a system like this, showing the user wrong info destroys trust immediately. It is way worse to tell a user that \"traffic deaths\" are the same thing as \"inflation\" than it is to miss a few marginal matches. The model is no longer too excited - its strict! Potential fix could be manually reviewing the grey area \"meh\" matches.\n",
    "\n",
    "**Future fix:** If we really want those 50 matches back, we could retrain the model on \"hard negatives\" (articles with the same date but different topics) so it learns to trust the text more than the calendar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74090db5",
   "metadata": {},
   "source": [
    "# Confidence Threshold Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f627fce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get probabilities for both\n",
    "probs_legacy = rf_legacy.predict_proba(X_test_legacy)[:, 1]\n",
    "probs_dutch = rf_dutch.predict_proba(X_test_dutch)[:, 1]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6), sharey=True)\n",
    "\n",
    "ax1.hist(probs_legacy, bins=50, color='lightcoral', edgecolor='black')\n",
    "ax1.set_title(\"Legacy Model A Decisiveness\")\n",
    "ax1.set_xlabel(\"Probability\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "\n",
    "ax2.hist(probs_dutch, bins=50, color='skyblue', edgecolor='black')\n",
    "ax2.set_title(\"Dutch Model B Decisiveness\")\n",
    "ax2.set_xlabel(\"Probability\")\n",
    "\n",
    "plt.suptitle(\"Comparison of Model Decisiveness\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9623658",
   "metadata": {},
   "source": [
    "# Hard Negative Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b06d221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for same-day pairs using Legacy test set\n",
    "X_hard_legacy = X_test_legacy[X_test_legacy['date_binary'] == 1]\n",
    "y_hard_legacy = y_test[X_test_legacy['date_binary'] == 1]\n",
    "\n",
    "# Predict using the Legacy model\n",
    "y_hard_pred_legacy = rf_legacy.predict(X_hard_legacy)\n",
    "\n",
    "print(\"--- Hard Negative Analysis: Legacy Model A (Same-Day Pairs Only) ---\")\n",
    "print(metrics.classification_report(y_hard_legacy, y_hard_pred_legacy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23495528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for same-day pairs using Dutch test set\n",
    "X_hard_dutch = X_test_dutch[X_test_dutch['date_binary'] == 1]\n",
    "y_hard_dutch = y_test[X_test_dutch['date_binary'] == 1]\n",
    "\n",
    "# Predict using the Dutch Vector model\n",
    "y_hard_pred_dutch = rf_dutch.predict(X_hard_dutch)\n",
    "\n",
    "print(\"--- Hard Negative Analysis: Dutch Model B (Same-Day Pairs Only) ---\")\n",
    "print(metrics.classification_report(y_hard_dutch, y_hard_pred_dutch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
